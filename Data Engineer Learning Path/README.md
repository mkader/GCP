Step	Activity	Duration	Type	Description

Step 1 Google Cloud Platform Big Data and Machine Learning Fundamentals (On-demand Training)
  1.1 Google Cloud Platform Big Data and Machine Learning Fundamentals 12	On-Demand	This 1-week accelerated on-demand course introduces participants to the Big Data and Machine Learning capabilities of Google Cloud Platform (GCP). It provides a quick overview of the Google Cloud Platform and a deeper dive of the data processing capabilities.

Step 2 Data Engineering on Google Cloud Platform (On-demand Training)
  2.1 Google Cloud Platform Big Data and Machine Learning Fundamentals 12	On-Demand	This 1-week accelerated on-demand course introduces participants to the Big Data and Machine Learning capabilities of Google Cloud Platform (GCP). It provides a quick overview of the Google Cloud Platform and a deeper dive of the data processing capabilities.
2.2 Modernizing Data Lakes and Data Warehouses with GCP	8	On-Demand	The two key components of any data pipeline are data lakes and warehouses. This course highlights use-cases for each type of storage and dives into the available data lake and warehouse solutions on Google Cloud Platform in technical detail. Also, this course describes the role of a data engineer, the benefits of a successful data pipeline to business operations, and examines why data engineering should be done in a cloud environment. Learners will get hands-on experience with data lakes and warehouses on Google Cloud Platform using QwikLabs.
2.3 Building Batch Data Pipelines on GCP	8	On-Demand	Data pipelines typically fall under one of the Extra-Load, Extract-Load-Transform or Extract-Transform-Load paradigms. This course describes which paradigm should be used and when for batch data. Furthermore, this course covers several technologies on Google Cloud Platform for data transformation including BigQuery, executing Spark on Cloud Dataproc, pipeline graphs in Cloud Data Fusion and serverless data processing with Cloud Dataflow. Learners will get hands-on experience building data pipeline components on Google Cloud Platform using QwikLabs.
2.4 Building Resilient Streaming Analytics Systems on GCP	8	On-Demand	Processing streaming data is becoming increasingly popular as streaming enables businesses to get real-time metrics on business operations. This course covers how to build streaming data pipelines on Google Cloud Platform. Cloud Pub/Sub is described for handling incoming streaming data. The course also covers how to apply aggregations and transformations to streaming data using Cloud Dataflow, and how to store processed records to BigQuery or Cloud Bigtable for analysis. Learners will get hands-on experience building streaming data pipeline components on Google Cloud Platform using QwikLabs.
2.5 Smart Analytics, Machine Learning, and AI on GCP	8	On-Demand	Incorporating machine learning into data pipelines increases the ability of businesses to extract insights from their data. This course covers several ways machine learning can be included in data pipelines on Google Cloud Platform depending on the level of customization required. For little to no customization, this course covers AutoML. For more tailored machine learning capabilities, this course introduces AI Platform Notebooks and BigQuery Machine Learning. Also, this course covers how to productionalize machine learning solutions using Kubeflow. Learners will get hands-on experience building machine learning models on Google Cloud Platform using QwikLabs.

Step 4 Create and Manage Cloud Resources (Qwiklabs Quest)
4.1 A Tour of Google Cloud Hands-on Labs	1	Lab	In this introductory-level lab, you will take your first steps with GCP by getting hands-on practice with the Google Cloud Platform consoleâ€”an in-browser UI that lets you access and manage Google Cloud services. You will identify key features of GCP and also learn the ins and outs of the Qwiklabs environment. If you are new to cloud computing or looking for an overview of GCP and Qwiklabs, you are in the right place. Read on to learn about the specifics of this lab and areas that you will get hands-on practice with.
4.2 Creating a Virtual Machine	1	Lab	In this hands-on lab you'll learn how to create virtual machine instances of various machine types using the Google Cloud Platform (GCP) Console and using the gcloud command line. You'll also learn how to connect an NGINX web server to your virtual machine.
4.2 Compute Engine: Qwik Start - Windows	1	Lab	In this hands-on lab, you will learn how to launch a Windows Server instance in Google Compute Engine, and connect to it using the Remote Desktop Protocol.
4.3 Getting Started with Cloud Shell and gcloud	1	Lab	In this hands-on lab you will learn how to connect to computing resources hosted on the Google Cloud Platform via Cloud Shell with the gcloud command-line.
4.4 Kubernetes Engine: Qwik Start	1	Lab	In this lab, you get hands on practice with container creation and application deployment with GKE.
4.5 Set Up Network and HTTP Load Balancers	1	Lab	In this hands-on lab, you'll learn the differences between a network load balancer and a HTTP load balancer, and how to set them up for your applications running on Google Compute Engine virtual machines.
4.6 Create and Manage Cloud Resources: Challenge Lab	1	Lab	This challenge lab tests your skills and knowledge from the labs in the Getting Started: Create and Manage Cloud Resources quest. You should be familiar with the content of labs before attempting this lab.

Step 5 Perform Foundational Data, ML, and AI Tasks in Google Cloud (Qwiklabs Quest)
5.1 AI Platform: Qwik Start	1	Lab	This lab will give you hands-on practice with TensorFlow 2.x model training, both locally and on AI Platform. After training, you will learn how to deploy your model to AI Platform for serving (prediction). You'll train your model to predict income category of a person using the United States Census Income Dataset. This lab gives you an introductory, end-to-end experience of training and prediction on AI Platform. The lab will use a census dataset to: Create a TensorFlow 2.x training application and validate it locally. Run your training job on a single worker instance in the cloud. Deploy a model to support prediction. Request an online prediction and see the response.
5.2 Dataprep: Qwik Start	1	Lab	Google Cloud Dataprep is an intelligent data service for visually exploring, cleaning, and preparing data for analysis. Cloud Dataprep is serverless and works at any scale. There is no infrastructure to deploy or manage. Easy data preparation with clicks and no code! In this lab you use Dataprep to manipulate a dataset. You import datasets, correct mismatched data, transform data, and join data. If this is new to you, you'll know what it all is by the end of this lab.
5.3 Dataflow: Qwik Start - Templates	0.75	Lab	In this lab, you will learn how to create a streaming pipeline using one of Google's Cloud Dataflow templates. More specifically, you will use the Cloud Pub/Sub to BigQuery template, which reads messages written in JSON from a Pub/Sub topic and pushes them to a BigQuery table. You can find the documentation for this template here. You'll be given the option to use the Cloud Shell command line or the Cloud Console to create the BigQuery dataset and table. Pick one method to use, then continue with that method for the rest of the lab. If you want experience using both methods, run through this lab a second time.
5.3 Dataflow: Qwik Start - Python	0.5	Lab	In this lab you will set up your Python development environment, get the Cloud Dataflow SDK for Python, and run an example pipeline using the Cloud Console.
5.4 Dataproc: Qwik Start - Console	0.5	Lab	Cloud Dataproc is a fast, easy-to-use, fully-managed cloud service for running Apache Spark and Apache Hadoop clusters in a simpler, more cost-efficient way. Operations that used to take hours or days take seconds or minutes instead. Create Cloud Dataproc clusters quickly and resize them at any time, so you don't have to worry about your data pipelines outgrowing your clusters. This lab shows you how to use the Google Cloud Console to create a Google Cloud Dataproc cluster, run a simple Apache Spark job in the cluster, then modify the number of workers in the cluster.
5.4 Dataproc: Qwik Start - Command Line	0.5	Lab	Cloud Dataproc is a fast, easy-to-use, fully-managed cloud service for running Apache Spark and Apache Hadoop clusters in a simpler, more cost-efficient way. Operations that used to take hours or days take seconds or minutes instead. Create Cloud Dataproc clusters quickly and resize them at any time, so you don't have to worry about your data pipelines outgrowing your clusters. This lab shows you how to use gcloud on the Google Cloud to create a Google Cloud Dataproc cluster, run a simple Apache Spark job in the cluster, then modify the number of workers in the cluster.
5.6 Cloud Natural Language API: Qwik Start	0.6	Lab	Cloud Natural Language API lets you extract information about people, places, events, (and more) mentioned in text documents, news articles, or blog posts. You can use it to understand sentiment about your product on social media, or parse intent from customer conversations happening in a call center or a messaging app. You can even upload text documents for analysis.
5.7 Google Cloud Speech API: Qwik Start	0.5	Lab	The Google Cloud Speech API enables easy integration of Google speech recognition technologies into developer applications. The Speech API allows you to send audio and receive a text transcription from the service (see What is the Google Cloud Speech API? for more information).
5.8 Video Intelligence: Qwik Start	0.5	Lab	Google Cloud Video Intelligence makes videos searchable and discoverable by extracting metadata with an easy to use REST API. You can now search every moment of every video file in your catalog. It quickly annotates videos stored in Cloud Storage, and helps you identify key entities (nouns) within your video; and when they occur within the video. Separate signal from noise by retrieving relevant information within the entire video, shot-by-shot, -or per frame.
5.9 Perform Foundational Data, ML, and AI Tasks in Google Cloud: Challenge Lab 1	Lab	You must complete a series of tasks within the allocated time period. Instead of following step-by-step instructions, you'll be given a scenario and a set of tasks - you figure out how to complete it on your own! An automated scoring system (shown on this page) will provide feedback on whether you have completed your tasks correctly. To score 100% you must complete all tasks within the time period! When you take a Challenge Lab, you will not be taught Google Cloud concepts. To build the solution to the challenge presented, use skills learned from the labs in the quest this challenge lab is part of. You will be expected to extend your learned skills; you will be expected to change default values, but new concepts will not be introduced. This lab is only recommended for students who have completed the labs in the Baseline: Data, ML, AI Quest.

Step 6 Engineer Data in Google Cloud (Qwiklabs Quest)
6.1 Creating a Data Transformation Pipeline with Cloud Dataprep 1.25	Lab	Cloud Dataprep by Trifacta is an intelligent data service for visually exploring, cleaning, and preparing structured and unstructured data for analysis. In this lab you explore the Cloud Dataprep UI to build a data transformation pipeline that runs at a scheduled interval and outputs results into BigQuery.
6.2 Building an IoT Analytics Pipeline on Google Cloud	1.15	Lab	The term Internet of Things (IoT) refers to the interconnection of physical devices with the global Internet. These devices are equipped with sensors and networking hardware, and each is globally identifiable. Taken together, these capabilities afford rich data about items in the physical world.
6.3 ETL Processing on Google Cloud Using Dataflow and BigQuery 1	Lab	In this lab you build several Data Pipelines that ingest data from a publicly available dataset into BigQuery, using these Google Cloud services: Cloud Storage Dataflow BigQuery You will create your own Data Pipeline, including the design considerations, as well as implementation details, to ensure that your prototype meets the requirements. Be sure to open the python files and read the comments when instructed to.
6.4 Predict Visitor Purchases with a Classification Model in BQML 1	Lab	In this lab you will use a newly available ecommerce dataset to run some typical queries that businesses would want to know about their customersâ€™ purchasing habits.
6.5 Predict Housing Prices with Tensorflow and AI Platform	1.5	Lab	In this lab, you will build an end to end machine learning solution using Tensorflow 1.x and AI Platform and leverage the cloud for distributed training and online prediction.
6.6 Cloud Composer: Copying BigQuery Tables Across Different Locations 1	Lab	In this advanced lab, you will learn how to create and run an Apache Airflow workflow in Cloud Composer that completes the following tasks: Reads from a config file the list of tables to copy Exports the list of tables from a BigQuery dataset located in US to Cloud Storage Copies the exported tables from US to EU Cloud Storage buckets Imports the list of tables into the target BigQuery Dataset in EU
6.7 Engineer Data in Google Cloud: Challenge Lab	1	Lab	You must complete a series of tasks within the allocated time period. Instead of following step-by-step instructions, you'll be given a scenario and a set of tasks - you figure out how to complete it on your own! An automated scoring system (shown on this page) will provide feedback on whether you have completed your tasks correctly.

Step 8 Professional Data Engineer Exam (Certification)
8.1 Professional Data Engineer	2	Certification	A Professional Data Engineer enables data-driven decision making by collecting, transforming, and publishing data. A Data Engineer should be able to design, build, operationalize, secure, and monitor data processing systems with a particular emphasis on security and compliance; scalability and efficiency; reliability and fidelity; and flexibility and portability. A Data Engineer should also be able to leverage, deploy, and continuously train pre-existing machine learning models.
